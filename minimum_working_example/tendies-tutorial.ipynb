{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving Image-Based Deep Learning Models with TensorFlow-Serving's RESTful API\n",
    "By Tyler LaBonte, July 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow-Serving is an incredibly handy tool that, due to its recency and rather niche use case, does not have much in the way of online tutorials. This Notebook is my solution to provide an end-to-end implementation of TensorFlow-Serving on an image-based model, clearly demonstrating everything from converting images to Base64 to integrating TensorFlow Model Server with a deep neural network (both of which are glossed over in Google's documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I will strive to provide a minimum working example -- this implementation can easily be extended to include Docker containers, Bazel builds, batched inferences, and model decoupling. The main focus here is to understand the general requirements for working with TensorFlow-Serving, independent of any optional bells and whistles. I will be using the RESTful version of TensorFlow-Serving as opposed to the gRPC version, and will implement the predict function, though classify and regress can also be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its most basic level, TensorFlow-Serving allows developers to integrate client requests and data with deep learning models served independently of client systems. Benefits of this include clients being able to make inferences on data without actually having to install TensorFlow or even have any contact with the actual model, and the ability to serve multiple clients with one instance of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline will look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pipeline_diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note especially that the image must pass from the client to the server as a Base64 encoded string. This is because JSON has no other way to represent images (besides an array representation of a tensor, and that gets out of hand very quickly). The image must also pass from the ProtoBuf to the Generator as a tensor. This can be modified, but it is best to keep any pre- and post-processing decoupled from the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting a TensorFlow model for serving is probably the most confusing part of the process, since there are a few steps involved.\n",
    "1. Export graph to ProtoBuf format. This saves the GraphDef and variables, and represents the trained model. In order to export an image-based model, we have to inject bitstring conversion layers into the beginning and ending of the graph, since we require our inference function to deal only in tensors.\n",
    "2. Wrap the ProtoBuf in a SavedModel. This step is necessary because TensorFlow-Serving's RESTful API is implemented through a SavedModelBuilder. We'll import our GraphDef, then extract the TensorInfo of the input and output to define our PREDICT signature definition.\n",
    "\n",
    "We'll use CycleGAN (https://github.com/tmlabonte/CycleGAN-TensorFlow) as a usage example. First, import some useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.insert(0, \"../CycleGAN-TensorFlow\")\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we instantiate a CycleGAN and inject our first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Instantiate a CycleGAN\n",
    "    cycle_gan = model.CycleGAN(ngf=64, norm=\"instance\", image_size=64)\n",
    "\n",
    "    # Create placeholder for image bitstring\n",
    "    # This is the injection of the input bitstring layer\n",
    "    input_bytes = tf.placeholder(tf.string, shape=[], name=\"input_bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we preprocess the bitstring to a float tensor batch so it can be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default(): \n",
    "    input_bytes = tf.reshape(input_bytes, [])\n",
    "    \n",
    "    # Transform bitstring to uint8 tensor\n",
    "    input_tensor = tf.image.decode_png(input_bytes, channels=3)\n",
    "    \n",
    "    # Convert to float32 tensor\n",
    "    input_tensor = tf.image.convert_image_dtype(input_tensor, dtype=tf.float32)\n",
    "    input_tensor = input_tensor / 127.5 - 1.0\n",
    "    \n",
    "    # Ensure tensor has correct shape\n",
    "    input_tensor = tf.reshape(input_tensor, [64, 64, 3])\n",
    "    \n",
    "    # CycleGAN's inference function accepts a batch of images\n",
    "    # So expand the single tensor into a batch of 1\n",
    "    input_tensor = tf.expand_dims(input_tensor, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we feed the tensor to the model and save its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Get style transferred tensor\n",
    "    output_tensor = cycle_gan.G.sample(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-inference, we convert the float tensor back to a bitstring. This is the injection of the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():    \n",
    "    # Convert to uint8 tensor\n",
    "    output_tensor = (output_tensor + 1.0) / 2.0\n",
    "    output_tensor = tf.image.convert_image_dtype(output_tensor, tf.uint8)\n",
    "    \n",
    "    # Remove the batch dimension\n",
    "    output_tensor = tf.squeeze(output_tensor, [0])\n",
    "    \n",
    "    # Transform uint8 tensor to bitstring\n",
    "    output_bytes = tf.image.encode_png(output_tensor)\n",
    "    output_bytes = tf.identity(output_bytes, name=\"output_bytes\")\n",
    "    \n",
    "    # Instantiate a Saver\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have injected the bitstring layers into our model, we will load our train checkpoints and save the graph as a ProtoBuf. Prior to coding this server, I trained CycleGAN for 10,000 steps and saved the checkpoint file on my local machine, which I access in this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../CycleGAN-TensorFlow/checkpoints/20180628-1208\\model.ckpt-10055\n",
      "INFO:tensorflow:Froze 52 variables.\n",
      "Converted 52 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "# Start a TensorFlow session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Access variables and weights from last checkpoint\n",
    "        latest_ckpt = tf.train.latest_checkpoint(\"../CycleGAN-TensorFlow/checkpoints/20180628-1208\")\n",
    "        saver.restore(sess, latest_ckpt)\n",
    "\n",
    "        # Export graph to ProtoBuf\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(sess, graph.as_graph_def(), [output_bytes.op.name])\n",
    "        tf.train.write_graph(output_graph_def, \"../CycleGAN-TensorFlow/protobufs\", \"model_v1\", as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we've completed step one! In step two, we will wrap the ProtoBuf in a SavedModel to use the RESTful API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate a SavedModelBuilder\n",
    "# Note that the serve directory is REQUIRED to have a model version subdirectory\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(\"serve/1\")\n",
    "\n",
    "# Read in ProtoBuf file\n",
    "with tf.gfile.GFile(\"../CycleGAN-TensorFlow/protobufs/model_v1\", \"rb\") as protobuf_file:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(protobuf_file.read())\n",
    "\n",
    "# Get input and output tensors from GraphDef\n",
    "# These are our injected bitstring layers\n",
    "[inp, out] = tf.import_graph_def(graph_def, name=\"\", return_elements=[\"input_bytes:0\", \"output_bytes:0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our signature definition, which expects the TensorInfo of the input and output to the model. When we save the model, we'll get a \"No assets\" message, but that's okay because our graph and variables were already saved in the ProtoBuf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'serve/1\\\\saved_model.pb'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'serve/1\\\\saved_model.pb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a TensorFlow session with our saved graph\n",
    "with tf.Session(graph=out.graph) as sess:\n",
    "        # Signature_definition expects a batch\n",
    "        # So we'll turn the output bitstring into a batch of 1 element\n",
    "        out = tf.expand_dims(out, 0)\n",
    "\n",
    "        # Build prototypes of input and output bitstrings\n",
    "        input_bytes = tf.saved_model.utils.build_tensor_info(inp)\n",
    "        output_bytes = tf.saved_model.utils.build_tensor_info(out)\n",
    "\n",
    "        # Create signature for prediction\n",
    "        signature_definition = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "            inputs={\"input_bytes\": input_bytes},\n",
    "            outputs={\"output_bytes\": output_bytes},\n",
    "            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "\n",
    "        # Add meta-information\n",
    "        builder.add_meta_graph_and_variables(\n",
    "            sess, [tf.saved_model.tag_constants.SERVING],\n",
    "            signature_def_map={\n",
    "                tf.saved_model.signature_constants.\n",
    "                DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_definition\n",
    "            })\n",
    "\n",
    "# Create the SavedModel\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We can run this TensorFlow Model Server from bash with the command:\n",
    "\n",
    "tensorflow_model_server --rest_api_port=8501 --model_name=saved_model --model_base_path=$(path)\n",
    "\n",
    "Where $(path) is the path to the serve directory. In my case, it is /mnt/c/Users/Tyler/Desktop/tendies/minimum_working_example/serve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client's job is to accept an image as input, convert it to Base64, pass it to the server using JSON, and decode the response. First, import some useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "import json\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be performing style transfer from an image of Gaussian noise to an image of sinusoidal noise. Here's the Gaussian image:\n",
    "<img src=\"images/gaussian1.png\" style=\"width: 256px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll open the image and convert it to base64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw bitstring: b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00' ... b'\\x00\\x00IEND\\xaeB`\\x82'\n",
      "Base64 encoded string: iVBORw0KGg ... 5ErkJggg==\n"
     ]
    }
   ],
   "source": [
    "# Open and read image as bitstring\n",
    "input_image = open(\"gaussian1.png\", \"rb\").read()\n",
    "print(\"Raw bitstring: \" + str(input_image[:10]) + \" ... \" + str(input_image[-10:]))\n",
    "\n",
    "# Encode image in b64\n",
    "encoded_input_string = base64.b64encode(input_image)\n",
    "input_string = encoded_input_string.decode(\"utf-8\")\n",
    "print(\"Base64 encoded string: \" + input_string[:10] + \" ... \" + input_string[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON data sent to our TensorFlow Model Server has to be structured in a very particular way. This method will be slightly different for classification and regression (see https://www.tensorflow.org/serving/api_rest). For image prediction calls, our JSON body must look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instances': [{'b64': 'iVBORw'}, {'b64': 'pT4rmN'}, {'b64': 'w0KGg2'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"instances\": [\n",
    "                  {\"b64\": \"iVBORw\"},\n",
    "                  {\"b64\": \"pT4rmN\"},\n",
    "                  {\"b64\": \"w0KGg2\"}\n",
    "                 ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're only sending one image to the server, it's pretty simple. We can create our JSON data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"instances\": [{\"b64\": \"iVBORw ... Jggg==\"}]}\n"
     ]
    }
   ],
   "source": [
    "# Wrap bitstring in JSON\n",
    "instance = [{\"b64\": input_string}]\n",
    "data = json.dumps({\"instances\": instance})\n",
    "print(data[:30] + \" ... \" + data[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all we need to send our POST request to the TensorFlow Model Server. This is a synchronous call, so the client will pause until it receives a response from the server (useful to know when you're wondering why your code has stopped after POSTing a very large image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_response = requests.post(\"http://localhost:8501/v1/models/saved_model:predict\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret the response, we do the above steps in the reverse order. To grab our base64-encoded image from the JSON response, we have to access:\n",
    "1. The value corresponding to \"predictions\" in the response dictionary.\n",
    "2. The first entry in the resultant array.\n",
    "3. The value corresponding to \"b64\" in the resultant dictionary.\n",
    "\n",
    "Then, we'll decode that value to a raw bitstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base64 encoded string: iVBORw0KGg ... 5ErkJggg==\n",
      "Raw bitstring: b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00' ... b'\\x00\\x00IEND\\xaeB`\\x82'\n"
     ]
    }
   ],
   "source": [
    "# Extract text from JSON\n",
    "response = json.loads(json_response.text)\n",
    "\n",
    "# Interpret bitstring output\n",
    "response_string = response[\"predictions\"][0][\"b64\"]\n",
    "print(\"Base64 encoded string: \" + response_string[:10] + \" ... \" + response_string[-10:])\n",
    "\n",
    "# Decode bitstring\n",
    "encoded_response_string = response_string.encode(\"utf-8\")\n",
    "response_image = base64.b64decode(encoded_response_string)\n",
    "print(\"Raw bitstring: \" + str(response_image[:10]) + \" ... \" + str(response_image[-10:]))\n",
    "\n",
    "# Save inferred image\n",
    "with open(\"sinusoidal1.png\", \"wb\") as output_file:\n",
    "    output_file.write(response_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Here's the resultant image of sinusoidal noise.\n",
    "<img src=\"images/sinusoidal1.png\" style=\"width: 256px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for following along with this tutorial; I hope it helped you out! This Notebook was built on the minimum working example of my TensorFlow Distributed Image Serving library, which you can download here: https://github.com/tmlabonte/tendies. For more blog posts and information about me, visit https://tmlabonte.github.io."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
