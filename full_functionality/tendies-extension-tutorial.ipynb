{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating TensorFlow Distributed Image Serving with the TensorFlow Object Detection API\n",
    "By Tyler LaBonte, July 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is a sequel to [Serving Image-Based Deep Learning Models with TensorFlow-Serving's RESTful API](https://github.com/tmlabonte/tendies/blob/master/minimum_working_example/tendies-basic-tutorial.ipynb). Please be sure to read that article to understand the basics of TensorFlow-Serving and the TensorFlow Distributed Image Serving (Tendies) library. It is highly recommended to clone [the Tendies repository](https://github.com/tmlabonte/tendies) for following along with this tutorial, as I will be focusing on important excerpts of code rather than the entire file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will extend the functionality of the basic Tendies classes to integrate a [Faster R-CNN](https://arxiv.org/abs/1506.01497) deep neural network, which uses the [TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection). This will allow us to serve the Faster R-CNN for REST-compliant remote inference, much like CycleGAN in the previous article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CycleGAN was rather simple because it accepted an image and output an image; however, Faster R-CNN accepts an image and outputs a dictionary of tensors. Furthermore, the Object Detection API forces us to build our model from a pipeline.config _and_ redefine the inference function, making serving Faster R-CNN a more difficult task. The steps to integrating a new model with Tendies are as follows:\n",
    "1. Define pre- and post-processing functions in LayerInjector.\n",
    "2. Create or import a model inference function in ServerBuilder.\n",
    "3. Create or import a client.\n",
    "\n",
    "While I will be demonstrating with Faster R-CNN, these steps remain the same for any arbitrary model, so feel free to follow along with your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pipeline_diagram.PNG\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each preprocessing function must take as arguments an image bitstring, channels, and \\*args (where \\*args can be used to represent any number of custom positional arguments), then returns the model input as a tensor. Conversely, each postprocessing function must take as arguments model output and \\*args, then return the list of output node names and whether the output should be transmitted as an image. These outputs will then be used in ServerBuilder when exporting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll define our preprocessing function, which will transform an image bitstring to a uint8 tensor suitable for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def bitstring_to_uint8_tensor(self, input_bytes, channels, *args):\n",
    "    input_bytes = tf.reshape(input_bytes, [])\n",
    "\n",
    "    # Transforms bitstring to uint8 tensor\n",
    "    input_tensor = tf.image.decode_png(input_bytes, channels=channels)\n",
    "\n",
    "    # Expands the single tensor into a batch of 1\n",
    "    input_tensor = tf.expand_dims(input_tensor, 0)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models compliant with the Object Detection API return a dictionary of useful tensors, such as num_detections, detection_boxes, and so on. In our postprocessing function, we'll iterate through these tensors and assign names to them, so we can extract them in ServerBuilder. We also have to account for the 1-indexing of the detection_classes tensor. Finally, we return a list of output node names and set output_as_image to False, since we will be sending the output tensors (not the visualized image) back to the client through JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection_dict_to_tensor_dict(self, object_detection_tensor_dict, *args):\n",
    "    # Sets output to a non-image\n",
    "    OUTPUT_AS_IMAGE = False\n",
    "    # Class labels are 1-indexed\n",
    "    LABEL_ID_OFFSET = 1\n",
    "\n",
    "    # Assigns names to tensors and adds them to output list\n",
    "    output_node_names = []\n",
    "    for name, tensor in object_detection_tensor_dict.items():\n",
    "        if name == \"detection_classes\":\n",
    "            tensor += LABEL_ID_OFFSET\n",
    "        tensor = tf.identity(tensor, name)\n",
    "        output_node_names.append(name)\n",
    "\n",
    "    # Returns output list and image boolean\n",
    "    return output_node_names, OUTPUT_AS_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are following along with your own model, feel free to utilize \\*args to accept as many parameters as you need for processing. Tendies is rather picky about tensor shape and typing, so make sure that the protos of the output of your preprocessor and the input of your postprocessor are equivalent to the input and output of your model, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to build the Faster R-CNN from pipeline.config and define our inference function. The code for this is in ServerBuilder.py under example_usage(), which is where the exportation of our model occurs. By reading the config file into an Object Detection API model_builder, we can instantiate a Faster R-CNN without ever actually seeing the model code. The next few cells are understood to be in the scope of example_usage()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.protos import pipeline_pb2\n",
    "from object_detection.builders import model_builder\n",
    "from google.protobuf import text_format\n",
    "\n",
    "# Builds object detection model from config file\n",
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.gfile.GFile(config_file_path, 'r') as config:\n",
    "    text_format.Merge(config.read(), pipeline_config)\n",
    "\n",
    "detection_model = model_builder.build(pipeline_config.model, is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since export_graph expects a single inference function, but the Object Detection API has its own pre- and post-processing to do, we have to combine them ourselves. This is a great place to use a closure, because we want to preserve the scope where we instantiated the Faster R-CNN as we pass around the inference function. [Closures are the best](https://i.imgflip.com/2en7d1.jpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates inference function, encapsulating object detection requirements\n",
    "def object_detection_inference(input_tensors):\n",
    "    # Converts uint8 inputs to float tensors\n",
    "    inputs = tf.to_float(input_tensors)\n",
    "\n",
    "    # Object detection preprocessing\n",
    "    preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)\n",
    "    # Object detection inference\n",
    "    output_tensors = detection_model.predict(preprocessed_inputs, true_image_shapes)\n",
    "    # Object detection postprocessing\n",
    "    postprocessed_tensors = detection_model.postprocess(output_tensors, true_image_shapes)\n",
    "    return postprocessed_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll instantiate a ServerBuilder and LayerInjector, then export the model. Note that we pass our inference function, preprocessor, and postprocessor into export_graph()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates a ServerBuilder\n",
    "server_builder = ServerBuilder()\n",
    "\n",
    "# Instantiates a LayerInjector\n",
    "layer_injector = LayerInjector()\n",
    "\n",
    "# Builds the server\n",
    "server_builder.build_server_from_tf(\n",
    "    inference_function=object_detection_inference,\n",
    "    preprocess_function=layer_injector.bitstring_to_uint8_tensor,\n",
    "    postprocess_function=layer_injector.object_detection_dict_to_tensor_dict,\n",
    "    model_name=FLAGS.model_name,\n",
    "    model_version=FLAGS.model_version,\n",
    "    checkpoint_dir=FLAGS.checkpoint_dir,\n",
    "    serve_dir=FLAGS.serve_dir,\n",
    "    channels=FLAGS.channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to create customized Tendies clients is to inherit from Client, which provides a framework for remote inference. In such a child class, one only has to create visualize() and associated helper functions, then call client.inference() to begin the evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a couple of these helper functions; the first is almost exactly the same as our preprocessing function, except without the addition of batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitstring_to_uint8_tensor(self, input_bytes):\n",
    "    input_bytes = tf.reshape(input_bytes, [])\n",
    "\n",
    "    # Transforms bitstring to uint8 tensor\n",
    "    input_tensor = tf.image.decode_jpeg(input_bytes, channels=3)\n",
    "\n",
    "    # Ensures tensor has correct shape\n",
    "    input_tensor = tf.reshape(input_tensor, [self.image_size, self.image_size, 3])\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second helper function will be used to create our category index dictionary from the Object Detection API from the provided label map; this specific implementation of Faster R-CNN has only one class, so it's simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "def get_category_index(self):\n",
    "    # Loads label map\n",
    "    label_map = label_map_util.load_labelmap(self.label_path)\n",
    "    \n",
    "    # Builds category index from label map\n",
    "    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=1, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    return category_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the use of our helpers, our visualize function isn't too bad. We'll decode the JSON data and convert it to bounding boxes, then overlay them on our input image with the help of the Object Detection API visualization_utils. Note that we convert the input image to a tensor, so we have to .eval() it before visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import visualization_utils\n",
    "def visualize(self, input_image, response, i):\n",
    "    # Processes response for visualization\n",
    "    detection_boxes = response[\"detection_boxes\"]\n",
    "    detection_classes = response[\"detection_classes\"]\n",
    "    detection_scores = response[\"detection_scores\"]\n",
    "    image = self.bitstring_to_uint8_tensor(input_image)\n",
    "    with tf.Session() as sess:\n",
    "        image = image.eval()\n",
    "\n",
    "    # Overlays bounding boxes and labels on image\n",
    "    visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image,\n",
    "        np.asarray(detection_boxes, dtype=np.float32),\n",
    "        np.asarray(detection_classes, dtype=np.uint8),\n",
    "        scores=np.asarray(detection_scores, dtype=np.float32),\n",
    "        category_index=self.get_category_index(),\n",
    "        instance_masks=None,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=2)\n",
    "\n",
    "    # Saves image\n",
    "    output_file = self.output_dir + \"/images/\" + self.output_filename + str(i) + self.output_extension\n",
    "    visualization_utils.save_image_array_as_png(image, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're done integrating Faster R-CNN with Tendies, let's run the server. First, we have to export our model:\n",
    "\n",
    "`python serverbuilder.py --checkpoint_dir $(path)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of July 2018, Python 3 is not officially supported with TensorFlow Serving, but [someone made a solution](https://github.com/tensorflow/serving/issues/700). Install the Python 3 TensorFlow Serving API with:\n",
    "\n",
    "`pip install tensorflow-serving-api-python3`\n",
    "\n",
    "Now, we can run this TensorFlow Model Server from bash with the command:\n",
    "\n",
    "`tensorflow_model_server --rest_api_port=8501 --model_name=saved_model --model_base_path=$(path)`\n",
    "\n",
    "Where $(path) is the path to the serve directory. In my case, it is /mnt/c/Users/Tyler/Desktop/tendies/full_functionality/serve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run remote inference by calling our client on a folder of input images:\n",
    "\n",
    "`python objectdetectionclient.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for following along with this tutorial; I hope it helped you out! This Notebook was built with my TensorFlow Distributed Image Serving library, which you can download [here](https://github.com/tmlabonte/tendies). For more blog posts and information about me, please visit [my website](https://tmlabonte.github.io)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
